# ğŸŒ€ API Data Ingestion with Kafka & Spark Streaming

This project demonstrates a real-time data ingestion pipeline using **Apache Kafka**, **Apache Spark Streaming**, and **Apache Airflow**. The data is extracted from a public **API**, sent to Kafka, then processed by Spark Streaming, and finally stored in **Hadoop HDFS**.

## ğŸ§± Architecture

![Architecture](assets/architecture.png)

## ğŸ›  Tech Stack

| Component | Technology |
|----------|-------------|
| Workflow Orchestration | Apache Airflow |
| Messaging | Apache Kafka |
| Streaming Consumer | Apache Spark |
| Coordination | Apache Zookeeper |
| Storage | Hadoop HDFS |
| Language | Python (Producer) |

---

## ğŸ“ Directory Structure

```bash
api-ingestion/
â”œâ”€â”€ airflow_dags/
â”‚   â””â”€â”€ api_to_kafka_dag.py
â”œâ”€â”€ kafka_producer/
â”‚   â””â”€â”€ producer.py
â”œâ”€â”€ spark_streaming/
â”‚   â””â”€â”€ spark_kafka_consumer.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ data/
â”‚   â””â”€â”€ output/ (saved HDFS results)
â””â”€â”€ README.md
```

## ğŸ“Œ Authors

- **Hung Nguyen** â€“ [@hungfnguyen](https://github.com/hungfnguyen)
